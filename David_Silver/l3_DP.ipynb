{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、动态规划\n",
    "Dynamic: sequential or temporal component to the problem\n",
    "Programming: optimising a program,i.e. a policy\n",
    "\n",
    "将复杂问题分解成子问题进行解决\n",
    "能使用动态规划需要问题有如下的两个性质：\n",
    "1. 最优解结构：可以分解为子问题，通过求解子问题的最优解进行解决\n",
    "2. overlapping subproblem 子问题出现多次，解可重复利用（cached and reused）->value function\n",
    "\n",
    "MDP符合以上性质，贝尔曼方程给出递归分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prediction and control\n",
    "**预测是“评估”，控制是“优化”。**\n",
    "\n",
    "---\n",
    "\n",
    "##### 一句话总结\n",
    "\n",
    "- **预测问题**：**给定一个策略，评估它的好坏。** （思想：学会“判断”）\n",
    "- **控制问题**：**在没有策略的前提下，寻找最佳策略。** （思想：学会“决策”）\n",
    "\n",
    "---\n",
    "\n",
    "##### 思想详解\n",
    "\n",
    "##### 1. 预测问题的思想\n",
    "- **核心目标**：理解在某个固定策略 `π` 下，每个状态（或状态-动作对）的长期价值 `Vπ(s)` 或 `Qπ(s, a)` 是多少。\n",
    "- **思想精髓**：这是一个 **“认知” 或 “评估”** 的过程。想象成你是一个顾问，老板给你一套固定的业务流程（策略），你的任务是分析并计算出按照这个流程，每个部门（状态）能产生多少长期效益（价值）。你并不改变流程，你只是理解它。\n",
    "- **解决方法**：策略评估。通过迭代应用贝尔曼期望方程，使价值估计收敛到该策略下的真实价值。\n",
    "\n",
    "##### 2. 控制问题的思想\n",
    "- **核心目标**：直接寻找能获得最大累积奖励的最优策略 `π*`，并同时得到最优价值函数 `V*(s)` 或 `Q*(s, a)`。\n",
    "- **思想精髓**：这是一个 **“改进” 或 “搜索”** 的过程。想象成你现在是老板本人，没有固定流程。你通过不断地“尝试-评估-改进”来探索所有可能的业务流程，最终找到那个能让公司总利润最大化的最佳流程（最优策略）。\n",
    "- **解决方法**：策略迭代（评估+改进）或价值迭代。其核心思想是**贪婪地在每个状态选择当前看来价值最高的动作**，从而逐步提升策略，直至无法再改进。\n",
    "\n",
    "### 两者的关系\n",
    "\n",
    "**控制问题建立在预测问题之上。** 我们通常通过反复解决预测问题（评估当前策略）来解决控制问题（改进出更好的策略）。\n",
    "\n",
    "这个“评估-改进”的循环，正是强化学习智能体从“知之甚少”到“精通 mastery”的完整学习思想。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Policy Evaluation\n",
    "Synchronous backups:\n",
    "- 对于每次迭代k+1\n",
    "- 对于状态空间中所有状态$s \\in S$\n",
    "- 从$v_k(s')$更新$v_{k+1}(s)$\n",
    "- 其中s'是s的后继\n",
    "\n",
    "$\\textbf{更新:} \\\\$\n",
    "$v^{k+1} = R^{\\pi} + \\gamma P^{\\pi}v^k$\n",
    "$$v_{k+1}(s) = \\sum_{a \\in A}\\pi(a|s)(R_s^{a} +\\gamma\\sum_{s' \\in S}P_{ss'}^{a}v_k(s')) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、Policy Iteration\n",
    "- Given a policy $\\pi$\n",
    "    - Evaluate the policy $\\pi$\n",
    "    $v_{\\pi}(s)$\n",
    "    - Improve the policy by acting greedily with respect to $v_{\\pi}$\n",
    "    $\\pi'=greedy(v_{pi})$\n",
    "    - 评估与改进循环进行\n",
    "\n",
    "- 如何根据v确定$\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Value Iteration\n",
    "其实没有理解这里面的贪心算法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Contraction Mapping"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
