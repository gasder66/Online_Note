{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、马尔可夫过程\n",
    "\n",
    "定义：随机过程中，未来状态的条件概率仅依赖于当前状态，与 “更早期” 的历史状态无关，即 “无后效性”。\n",
    "\n",
    "数学表达：对任意时刻$t_n < t_{n+1} < \\dots < t_m$，有$P(X_{t_m}=s_m | X_{t_{m-1}}=s_{m-1},\\dots,X_{t_n}=s_n) = P(X_{t_m}=s_m | X_{t_{m-1}}=s_{m-1})$（$s_i$为状态）。\n",
    "核心：当前状态已包含预测未来所需的全部信息，历史状态不额外提供价值。\n",
    "\n",
    "##### 马尔可夫转移矩阵\n",
    "\n",
    "定义：描述马尔可夫过程中 “从一个状态转移到另一个状态的概率” 的矩阵，记为$P = [p_{ij}]$。$\\newline$\n",
    "行：代表 “当前状态”（出发状态）；$\\newline$\n",
    "列：代表 “下一状态”（到达状态）；$\\newline$\n",
    "元素$p_{ij} = P(X_{t+1}=j | X_t=i)$，即从状态$i$转移到状态$j$的概率。$\\newline$\n",
    "关键性质：$\\newline$\n",
    "每行元素之和为 1（概率归一性）：$\\sum_{j} p_{ij} = 1$（从任一状态出发，转移到所有可能状态的概率总和为 1）；$\\newline$\n",
    "元素非负：$p_{ij} \\geq 0$（概率非负）。$\\newline$\n",
    "与马尔可夫性质的关联：转移矩阵的元素定义基于马尔可夫性质，仅需当前状态即可确定转移概率，无需历史信息。\n",
    "\n",
    "##### 定义\n",
    "一个马尔可夫过程的定义：二元组（S,P），S表示所有状态的集合，P表示马尔可夫转移概率矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、马尔可夫奖励过程\n",
    "\n",
    "##### 定义\n",
    "元组$(S, P, R, \\gamma) \\newline$\n",
    "S是一个有限的状态集合 $\\newline$\n",
    "P 是状态转移概率矩阵，$P_{ss^{'}}=P[S_{t+1}=s^{'} | S_t=s]$ $\\newline$\n",
    "R 是奖励函数 $R_s = E[R_{t+1} | S_t=s] \\newline$\n",
    "$\\gamma$是折扣因子\n",
    "\n",
    "$G_t = \\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1} \\newline$\n",
    "$\\gamma$：现在我有多在乎未来的奖励 $\\newline$\n",
    "好处：\n",
    "- 避免这个级数发散\n",
    "- 未来具有不确定性\n",
    "- 金融上利率和通胀等\n",
    "\n",
    "##### 价值函数\n",
    "$v(s) = E[G_t | S_t=s]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bellman Equation for MRPs\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "v(s) &= E[G_t | S_t = s] \\\\\n",
    "&= E[R_{t+1} + \\gamma(R_{t+2} + \\gamma R_{t+3} + \\dots)|S_t=s] \\\\\n",
    "&= E[R_{t+1} + \\gamma G_{t+1}|S_t=s] \\\\\n",
    "&= E[R_{t+1} + \\gamma v(S_{t+1}) | S_t=s]\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "##### 矩阵形式\n",
    "$v =R+\\gamma Pv$\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "\\nu(1) \\\\ \n",
    "\\vdots \\\\ \n",
    "\\nu(n) \n",
    "\\end{bmatrix} \n",
    "= \n",
    "\\begin{bmatrix} \n",
    "\\mathcal{R}_1 \\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathcal{R}_n \n",
    "\\end{bmatrix} \n",
    "+ \\gamma\n",
    "\\begin{bmatrix} \n",
    "\\mathcal{P}_{11} & \\dots & \\mathcal{P}_{1n} \\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "\\mathcal{P}_{n1} & \\dots & \\mathcal{P}_{nn} \n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix} \n",
    "\\nu(1) \\\\ \n",
    "\\vdots \\\\ \n",
    "\\nu(n) \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三、MDP马尔可夫决策过程\n",
    "\n",
    "#### 定义\n",
    "元组$(S, A, P, R, \\gamma) \\newline$\n",
    "S是一个有限的状态集合 $\\newline$\n",
    "A是一个有限的动作集合 $\\\\$\n",
    "P 是状态转移概率矩阵，$P^{a}_{ss^{'}}=P[S_{t+1}=s^{'} | S_t=s, A_t=a]$ $\\newline$\n",
    "R 是奖励函数 $R^{a}_s = E[R_{t+1} | S_t=s, A_t=a] \\newline$\n",
    "$\\gamma$是折扣因子\n",
    "\n",
    "#### policy策略\n",
    "$\\pi(a|s) = P[A_t=a|S_t=s]$\n",
    "策略完全决定了一个智能体的行为\n",
    "\n",
    "固定策略，则MDP规约为MRP\n",
    "\n",
    "*   如果我们只观察智能体根据策略 $\\pi$ 行动时所经历的状态序列 $S_1, S_2, \\ldots$，那么这个序列本身就是一个马尔可夫过程。\n",
    "*   它只包含**状态集合 S** 和由策略 $\\pi$ 决定的**状态转移概率 $P^\\pi$**。\n",
    "\n",
    "*   如果我们观察状态以及伴随的奖励序列 $S_1, R_2, S_2, R_3, \\ldots$，那么这个序列就是一个马尔可夫奖励过程。\n",
    "*   它在马尔可夫过程的基础上，增加了由策略 $\\pi$ 决定的**期望奖励 $R^\\pi$** 和**折扣因子 $\\gamma$**。\n",
    "\n",
    "\n",
    "**在策略 $\\pi$ 下的状态转移概率**：\n",
    "$$\n",
    "P_{s,s'}^\\pi = \\sum_{a \\in A} \\pi(a|s) P_{s,s'}^a\n",
    "$$\n",
    "*   **解释**：从状态 $s$ 转移到状态 $s'$ 的概率，是在 $s$ 下所有可能动作 $a$ 的转移概率的加权平均。权重就是策略 $\\pi$ 选择每个动作 $a$ 的概率 $\\pi(a|s)$。\n",
    "$ P^{a}_{s,s'}$ 表示在马尔可夫决策过程（MDP）中，**在状态 $s$ 执行动作 $a$ 后，转移到状态 $s'$ 的概率**。\n",
    "\n",
    "**在策略 $\\pi$ 下的期望奖励**：\n",
    "$$\n",
    "R_s^\\pi = \\sum_{a \\in A} \\pi(a|s) R_s^a\n",
    "$$\n",
    "*   **解释**：在状态 $s$ 下获得的期望即时奖励，是在 $s$ 下所有可能动作 $a$ 的期望奖励的加权平均。权重同样是策略 $\\pi$ 选择每个动作的概率。这里的 $R_s^a$ 是 MDP 中定义的在状态 $s$ 执行动作 $a$ 的期望奖励。\n",
    "\n",
    "#### Value Function\n",
    "$$q_{\\pi}(s,a) = E_[G_t | S_t=s,A_t=a]$$\n",
    "\n",
    "#### Bellman Equation for $q_{\\pi}$\n",
    "$$q_{\\pi}(s,a) = R_s^a + \\gamma \\sum_{s' \\in S}P_{ss'}^{a} \\sum_{a' \\in A} \\pi(a'|s')q_{\\pi}(s',a')$$\n",
    "\n",
    "optimal state_value function $\\\\$\n",
    "$v_*(s) = \\max v_{\\pi}(s)$\n",
    "optimal policy最优策略 $\\\\$\n",
    "为策略定义一个偏序关系：\n",
    "$\\pi \\geq \\pi' \\textbf{if} v_{\\pi}(s) \\geq v_{\\pi'}(s) \\forall s$\n",
    "对于任意的MDP，总是存在一个最优的策略\n",
    "\n",
    "一个最优策略由最大化 $q_n(s, a)$得到,\n",
    "\n",
    "$$\n",
    "\\pi_x(a|s) =\n",
    "\\begin{cases} \n",
    "1 & \\text{if } a = \\operatorname*{argmax}_{a \\in A} q_n(s, a) \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**核心区别点睛：**\n",
    "MRP是描述环境的**被动模型**，回答“这个状态有多好？”；而MDP是智能体的**决策框架**，通过动作和策略回答“在此状态下我该做什么？”。简言之，MRP关乎**状态价值评估**，MDP关乎**最优决策制定**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "辨析V与R\n",
    "\n",
    "**R 是眼前的“小钱”，V 是未来的“总身家”。**\n",
    "\n",
    "- **R(s) 或 R(s,a)** 是**即时奖励**：代表在**当前这一步**，进入状态s或执行动作a后，环境直接给你的现金。\n",
    "- **V(s)** 是**状态价值**：代表从状态s出发，考虑到**未来所有可能的路径和所有奖励**，你最终能获得的全部财富的折现总和。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1. 在马尔可夫奖励过程（MRP）中计算 $V(s)$\n",
    "\n",
    "在 MRP 中，由于没有动作的概念，我们直接计算每个状态的价值。\n",
    "\n",
    "##### 贝尔曼方程（MRP）\n",
    "$$\n",
    "V(s) = R_s + \\gamma \\sum_{s' \\in S} P_{ss'} V(s')\n",
    "$$\n",
    "其中：\n",
    "- $R_s$：在状态 $s$ 的即时奖励\n",
    "- $\\gamma$：折扣因子（0 ≤ γ ≤ 1）\n",
    "- $P_{ss'}$：从状态 $s$ 转移到状态 $s'$ 的概率\n",
    "- $V(s')$：下一个状态 $s'$ 的价值\n",
    "\n",
    "这个方程的意思是：**状态 $s$ 的价值 = 即时奖励 + 所有可能下一个状态的折扣后价值的期望**。\n",
    "\n",
    "\n",
    "#### 2. 在马尔可夫决策过程（MDP）中计算 $V_\\pi(s)$\n",
    "\n",
    "在 MDP 中，状态价值函数依赖于策略 $\\pi$，记为 $V_\\pi(s)$。\n",
    "\n",
    "##### 贝尔曼期望方程（MDP）\n",
    "$$\n",
    "V_\\pi(s) = \\sum_{a \\in A} \\pi(a|s) \\left[ R(s, a) + \\gamma \\sum_{s' \\in S} P(s'|s, a) V_\\pi(s') \\right]\n",
    "$$\n",
    "其中：\n",
    "- $\\pi(a|s)$：在状态 $s$ 下选择动作 $a$ 的概率\n",
    "- $R(s, a)$：在状态 $s$ 下执行动作 $a$ 的即时奖励\n",
    "- $P(s'|s, a)$：在状态 $s$ 下执行动作 $a$ 后转移到状态 $s'$ 的概率\n",
    "\n",
    "这个方程的意思是：**在策略 $\\pi$ 下，状态 $s$ 的价值 = 所有可能动作的价值的期望，其中每个动作的价值 = 即时奖励 + 所有可能下一个状态的折扣后价值的期望**。\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
